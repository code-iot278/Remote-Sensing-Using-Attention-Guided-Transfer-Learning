# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G8FJjNCbrx4rua1KCRKO8QHBLd09BVnT

# **Access Drive File**
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **Proposed Model**"""

# =========================================================
# FULL END-TO-END PROPOSED MODEL (UPDATED)
#
# =========================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
import os, time
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# ---------------------------------------------------------
# MBConv Block
# ---------------------------------------------------------
class MBConv(nn.Module):
    def __init__(self, in_ch, out_ch, expansion=4):
        super().__init__()
        mid = in_ch * expansion
        self.block = nn.Sequential(
            nn.Conv2d(in_ch, mid, 1, bias=False),
            nn.BatchNorm2d(mid),
            nn.ReLU6(inplace=True),

            nn.Conv2d(mid, mid, 3, padding=1, groups=mid, bias=False),
            nn.BatchNorm2d(mid),
            nn.ReLU6(inplace=True),

            nn.Conv2d(mid, out_ch, 1, bias=False),
            nn.BatchNorm2d(out_ch)
        )

    def forward(self, x):
        return self.block(x)

# ---------------------------------------------------------
# Attention Map Generator
# ---------------------------------------------------------
class AttentionMap(nn.Module):
    def __init__(self, ch):
        super().__init__()
        self.conv = nn.Conv2d(ch * 2, ch, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, f1, f2):
        return self.sigmoid(self.conv(torch.cat([f1, f2], dim=1)))

# ---------------------------------------------------------
# Shallow Attention Block
# ---------------------------------------------------------
class ShallowAttention(nn.Module):
    def __init__(self, ch):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv2d(ch, ch, 3, padding=1),
            nn.BatchNorm2d(ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.block(x)

# ---------------------------------------------------------
# Cross-Attention Feature Interaction
# ---------------------------------------------------------
class CrossAttentionInteraction(nn.Module):
    def __init__(self, ch):
        super().__init__()
        self.attn = AttentionMap(ch)
        self.fc1 = nn.Conv2d(ch, ch, 1)
        self.fc2 = nn.Conv2d(ch, ch, 1)

        self.sa1 = ShallowAttention(ch)
        self.sa2 = ShallowAttention(ch)
        self.sa3 = ShallowAttention(ch)

        self.reduce = nn.Conv2d(ch * 3, ch, 1)

    def forward(self, f1, f2):
        A = self.attn(f1, f2)

        f1 = self.fc1(f1 * A)
        f2 = self.fc2(f2 * A)

        s1 = self.sa1(f1)
        s2 = self.sa2(f2)
        s3 = self.sa3(f1 + f2)

        fused = torch.cat([s1, s2, s3], dim=1)
        out = self.reduce(fused)

        return out, out

# ---------------------------------------------------------
# Temporal Feature Fusion
# ---------------------------------------------------------
class TemporalFusion(nn.Module):
    def __init__(self, ch):
        super().__init__()
        self.conv = nn.Conv2d(ch * 2, ch, 3, padding=1)

    def forward(self, f1, f2):
        return self.conv(torch.cat([f1, f2], dim=1))

# ---------------------------------------------------------
# Attention Module (AM)
# ---------------------------------------------------------
class AttentionModule(nn.Module):
    def __init__(self, ch):
        super().__init__()
        self.conv = nn.Conv2d(ch, 1, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        return x * self.sigmoid(self.conv(x))

# ---------------------------------------------------------
# Squeeze-and-Excitation Block
# ---------------------------------------------------------
class SEBlock(nn.Module):
    def __init__(self, ch, r=16):
        super().__init__()
        self.fc1 = nn.Linear(ch, ch // r)
        self.fc2 = nn.Linear(ch // r, ch)

    def forward(self, x):
        b, c, _, _ = x.shape
        y = F.adaptive_avg_pool2d(x, 1).view(b, c)
        y = torch.relu(self.fc1(y))
        y = torch.sigmoid(self.fc2(y)).view(b, c, 1, 1)
        return x * y

# ---------------------------------------------------------
# FULL PROPOSED NETWORK
# ---------------------------------------------------------
class ProposedModel(nn.Module):
    def __init__(self, in_ch=3, base_ch=64):
        super().__init__()

        self.mb1a = MBConv(in_ch, base_ch)
        self.mb1b = MBConv(in_ch, base_ch)

        self.ca1 = CrossAttentionInteraction(base_ch)
        self.mb2a = MBConv(base_ch, base_ch)
        self.mb2b = MBConv(base_ch, base_ch)

        self.ca2 = CrossAttentionInteraction(base_ch)
        self.mb3a = MBConv(base_ch, base_ch)
        self.mb3b = MBConv(base_ch, base_ch)

        self.temporal = TemporalFusion(base_ch)



        self.se = SEBlock(base_ch)

        # Shared Attention Module
        self.am = AttentionModule(base_ch)

        self.se = SEBlock(base_ch)
        # Shared Attention Module
        self.am = AttentionModule(base_ch)
        self.se = SEBlock(base_ch)
        self.output = nn.Conv2d(base_ch, in_ch, 1)

    def forward(self, x1, x2):
        # Initial feature extraction
        f1 = self.mb1a(x1)
        f2 = self.mb1b(x2)

        # CA1 → AM
        f1, f2 = self.ca1(f1, f2)
        f1 = self.am(f1)
        f2 = self.am(f2)

        f1 = self.mb2a(f1)
        f2 = self.mb2b(f2)

        # CA2 → AM
        f1, f2 = self.ca2(f1, f2)
        f1 = self.am(f1)
        f2 = self.am(f2)

        f1 = self.mb3a(f1)
        f2 = self.mb3b(f2)

        # Temporal Fusion → AM → SE
        x = self.temporal(f1, f2)
        x = self.se(x)

        x = self.am(x)


        return self.output(x)

# ---------------------------------------------------------
# TEST RUN
# ---------------------------------------------------------
if __name__ == "__main__":
    model = ProposedModel()
    x1 = torch.randn(1, 3, 224, 224)
    x2 = torch.randn(1, 3, 224, 224)
    y = model(x1, x2)
# =========================================================
# 1. DATASET PATHS
# =========================================================
dataset1_path = "/content/drive/MyDrive/Colab Notebooks/Remote Sensing Data.v2i.yolov8"
dataset2_path = "/content/drive/MyDrive/Colab Notebooks/archive (88)/png"

img_size = (224, 224)
batch_size = 16
epochs = 1  # Change as needed

# =========================================================
# 2. DATA GENERATORS
# =========================================================
datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

def load_data(path):
    train = datagen.flow_from_directory(
        path,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical',
        subset='training',
        shuffle=True
    )
    val = datagen.flow_from_directory(
        path,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical',
        subset='validation',
        shuffle=False
    )
    return train, val

train1, val1 = load_data(dataset1_path)
train2, val2 = load_data(dataset2_path)

class_names1 = list(train1.class_indices.keys())
class_names2 = list(train2.class_indices.keys())

# =========================================================
# 3. DISPLAY SAMPLE INPUT IMAGES (DATASET-1)
# =========================================================
x_sample, _ = next(train1)
num_images = min(4, x_sample.shape[0])
plt.figure(figsize=(8,4))
for i in range(num_images):
    plt.subplot(1,num_images,i+1)
    plt.imshow(x_sample[i])
    plt.axis("off")
plt.suptitle("Dataset-1: Sample Input Images")
plt.show()

# =========================================================
# 4. VGG16 FEATURE EXTRACTION MODEL FOR DATASET-1
# =========================================================
base_model = VGG16(weights="imagenet", include_top=False, input_shape=(224,224,3))

x = GlobalAveragePooling2D()(base_model.output)
output1 = Dense(train1.num_classes, activation="softmax")(x)

model1 = Model(inputs=base_model.input, outputs=output1)
model1.compile(optimizer=Adam(0.0001), loss="categorical_crossentropy", metrics=["accuracy"])

# =========================================================
# 5. TRAIN MODEL ON DATASET-1
# =========================================================
start_time = time.time()
history1 = model1.fit(train1, validation_data=val1, epochs=epochs, verbose=1)
end_time = time.time()

fps1 = train1.samples / (end_time - start_time)
pn_million = model1.count_params() / 1e6

# =========================================================
# 6. FEATURE MAP VISUALIZATION FUNCTION
# =========================================================
def visualize_feature_maps(x_batch, model_layer, max_channels=6):
    feature_model = Model(inputs=base_model.input, outputs=model_layer.output)
    for idx in range(x_batch.shape[0]):
        img = x_batch[idx:idx+1]
        feature_maps = feature_model.predict(img)
        num_channels = min(max_channels, feature_maps.shape[-1])
        plt.figure(figsize=(15,3))
        for i in range(num_channels):
            plt.subplot(1, num_channels, i+1)
            plt.imshow(feature_maps[0,:,:,i], cmap="viridis")
            plt.axis("off")
        plt.suptitle(f"Image {idx+1} Feature Maps (first {num_channels} channels)")
        plt.show()

# =========================================================
# 7. DISPLAY INPUT + OUTPUT + METRICS FUNCTION
# =========================================================
def display_results(x_gen, model, class_names, dataset_name, max_images=4, max_channels=6):
    if x_gen.samples == 0:
        print(f"\n===== {dataset_name} has no images! Skipping =====\n")
        return 0, 0, 0, 0

    x_batch, y_batch = next(x_gen)
    preds = model.predict(x_batch, steps=None)
    y_true = np.argmax(y_batch, axis=1)
    y_pred = np.argmax(preds, axis=1)

    acc  = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, average="macro", zero_division=0)
    rec  = recall_score(y_true, y_pred, average="macro", zero_division=0)
    f1s  = f1_score(y_true, y_pred, average="macro", zero_division=0)

    num_images = min(max_images, x_batch.shape[0])
    plt.figure(figsize=(12,4))
    for i in range(num_images):
        plt.subplot(1, num_images, i+1)
        plt.imshow(x_batch[i])
        plt.title(f"GT: {class_names[y_true[i]]}\nPred: {class_names[y_pred[i]]}")
        plt.axis("off")
    plt.suptitle(f"{dataset_name}: Input vs Output")
    plt.show()

    print(f"\n===== {dataset_name} METRICS =====")
    print(f"Accuracy (%)  : {acc*100:.2f}")
    print(f"Precision (%) : {prec*100:.2f}")
    print(f"Recall (%)    : {rec*100:.2f}")
    print(f"F1-score (%)  : {f1s*100:.2f}")
    print("================================\n")

    visualize_feature_maps(x_batch[:num_images], base_model.layers[3], max_channels=max_channels)
    return acc, prec, rec, f1s

# =========================================================
# 8. DISPLAY RESULTS – DATASET-1
# =========================================================
acc1, prec1, rec1, f11 = display_results(val1, model1, class_names1, "DATASET-1")

# =========================================================
# 9. VGG16 MODEL FOR DATASET-2 (SEPARATE IF CLASS COUNT DIFFERS)
# =========================================================
x2 = GlobalAveragePooling2D()(base_model.output)
output2 = Dense(train2.num_classes, activation="softmax")(x2)
model2 = Model(inputs=base_model.input, outputs=output2)
model2.compile(optimizer=Adam(0.0001), loss="categorical_crossentropy", metrics=["accuracy"])

# =========================================================
# 10. TRAIN MODEL ON DATASET-2
# =========================================================
history2 = model2.fit(train2, validation_data=val2, epochs=epochs, verbose=1)

# =========================================================
# 11. DISPLAY RESULTS – DATASET-2
# =========================================================
acc2, prec2, rec2, f12 = display_results(val2, model2, class_names2, "DATASET-2")

# =========================================================
# 12. FINAL SUMMARY METRICS
# =========================================================
print("\n=========== FINAL SUMMARY ===========")
print("Metric        Dataset-1    Dataset-2")
print(f"Accuracy      {acc1*100:.2f}        {acc2*100:.2f}")
print(f"Precision     {prec1*100:.2f}        {prec2*100:.2f}")
print(f"Recall        {rec1*100:.2f}        {rec2*100:.2f}")
print(f"F1-score      {f11*100:.2f}        {f12*100:.2f}")
print("------------------------------------")

print(f"PN (M)        {pn_million:.2f}")
print(f"FPS           {fps1:.2f}")
print(f"Train Loss 1  {history1.history['loss'][-1]:.4f}")
print(f"Val Loss 1    {history1.history['val_loss'][-1]:.4f}")
print(f"Train Loss 2  {history2.history['loss'][-1]:.4f}")
print(f"Val Loss 2    {history2.history['val_loss'][-1]:.4f}")
print("====================================")

"""# **VGG16 FEATURE EXTRACTION MODEL**"""

# =========================================================
# VGG16 FEATURE EXTRACTION – TWO DATASETS – FULL PIPELINE
# TRAIN + EVALUATE + PER-IMAGE FEATURE MAP VISUALIZATION
# =========================================================

import os, time
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# =========================================================
# 1. DATASET PATHS
# =========================================================
dataset1_path = "/content/drive/MyDrive/Colab Notebooks/Remote Sensing Data.v2i.yolov8"
dataset2_path = "/content/drive/MyDrive/Colab Notebooks/archive (88)/png"

img_size = (224, 224)
batch_size = 16
epochs = 1  # Change as needed

# =========================================================
# 2. DATA GENERATORS
# =========================================================
datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

def load_data(path):
    train = datagen.flow_from_directory(
        path,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical',
        subset='training',
        shuffle=True
    )
    val = datagen.flow_from_directory(
        path,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical',
        subset='validation',
        shuffle=False
    )
    return train, val

train1, val1 = load_data(dataset1_path)
train2, val2 = load_data(dataset2_path)

class_names1 = list(train1.class_indices.keys())
class_names2 = list(train2.class_indices.keys())

# =========================================================
# 3. DISPLAY SAMPLE INPUT IMAGES (DATASET-1)
# =========================================================
x_sample, _ = next(train1)
num_images = min(4, x_sample.shape[0])
plt.figure(figsize=(8,4))
for i in range(num_images):
    plt.subplot(1,num_images,i+1)
    plt.imshow(x_sample[i])
    plt.axis("off")
plt.suptitle("Dataset-1: Sample Input Images")
plt.show()

# =========================================================
# 4. VGG16 FEATURE EXTRACTION MODEL FOR DATASET-1
# =========================================================
base_model = VGG16(weights="imagenet", include_top=False, input_shape=(224,224,3))

x = GlobalAveragePooling2D()(base_model.output)
output1 = Dense(train1.num_classes, activation="softmax")(x)

model1 = Model(inputs=base_model.input, outputs=output1)
model1.compile(optimizer=Adam(0.0001), loss="categorical_crossentropy", metrics=["accuracy"])

# =========================================================
# 5. TRAIN MODEL ON DATASET-1
# =========================================================
start_time = time.time()
history1 = model1.fit(train1, validation_data=val1, epochs=epochs, verbose=1)
end_time = time.time()

fps1 = train1.samples / (end_time - start_time)
pn_million = model1.count_params() / 1e6

# =========================================================
# 6. FEATURE MAP VISUALIZATION FUNCTION
# =========================================================
def visualize_feature_maps(x_batch, model_layer, max_channels=6):
    feature_model = Model(inputs=base_model.input, outputs=model_layer.output)
    for idx in range(x_batch.shape[0]):
        img = x_batch[idx:idx+1]
        feature_maps = feature_model.predict(img)
        num_channels = min(max_channels, feature_maps.shape[-1])
        plt.figure(figsize=(15,3))
        for i in range(num_channels):
            plt.subplot(1, num_channels, i+1)
            plt.imshow(feature_maps[0,:,:,i], cmap="viridis")
            plt.axis("off")
        plt.suptitle(f"Image {idx+1} Feature Maps (first {num_channels} channels)")
        plt.show()

# =========================================================
# 7. DISPLAY INPUT + OUTPUT + METRICS FUNCTION
# =========================================================
def display_results(x_gen, model, class_names, dataset_name, max_images=4, max_channels=6):
    if x_gen.samples == 0:
        print(f"\n===== {dataset_name} has no images! Skipping =====\n")
        return 0, 0, 0, 0

    x_batch, y_batch = next(x_gen)
    preds = model.predict(x_batch, steps=None)
    y_true = np.argmax(y_batch, axis=1)
    y_pred = np.argmax(preds, axis=1)

    acc  = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, average="macro", zero_division=0)
    rec  = recall_score(y_true, y_pred, average="macro", zero_division=0)
    f1s  = f1_score(y_true, y_pred, average="macro", zero_division=0)

    num_images = min(max_images, x_batch.shape[0])
    plt.figure(figsize=(12,4))
    for i in range(num_images):
        plt.subplot(1, num_images, i+1)
        plt.imshow(x_batch[i])
        plt.title(f"GT: {class_names[y_true[i]]}\nPred: {class_names[y_pred[i]]}")
        plt.axis("off")
    plt.suptitle(f"{dataset_name}: Input vs Output")
    plt.show()

    print(f"\n===== {dataset_name} METRICS =====")
    print(f"Accuracy (%)  : {acc*100:.2f}")
    print(f"Precision (%) : {prec*100:.2f}")
    print(f"Recall (%)    : {rec*100:.2f}")
    print(f"F1-score (%)  : {f1s*100:.2f}")
    print("================================\n")

    visualize_feature_maps(x_batch[:num_images], base_model.layers[3], max_channels=max_channels)
    return acc, prec, rec, f1s

# =========================================================
# 8. DISPLAY RESULTS – DATASET-1
# =========================================================
acc1, prec1, rec1, f11 = display_results(val1, model1, class_names1, "DATASET-1")

# =========================================================
# 9. VGG16 MODEL FOR DATASET-2 (SEPARATE IF CLASS COUNT DIFFERS)
# =========================================================
x2 = GlobalAveragePooling2D()(base_model.output)
output2 = Dense(train2.num_classes, activation="softmax")(x2)
model2 = Model(inputs=base_model.input, outputs=output2)
model2.compile(optimizer=Adam(0.0001), loss="categorical_crossentropy", metrics=["accuracy"])

# =========================================================
# 10. TRAIN MODEL ON DATASET-2
# =========================================================
history2 = model2.fit(train2, validation_data=val2, epochs=epochs, verbose=1)

# =========================================================
# 11. DISPLAY RESULTS – DATASET-2
# =========================================================
acc2, prec2, rec2, f12 = display_results(val2, model2, class_names2, "DATASET-2")

# =========================================================
# 12. FINAL SUMMARY METRICS
# =========================================================
print("\n=========== FINAL SUMMARY ===========")
print("Metric        Dataset-1    Dataset-2")
print(f"Accuracy      {acc1*100:.2f}        {acc2*100:.2f}")
print(f"Precision     {prec1*100:.2f}        {prec2*100:.2f}")
print(f"Recall        {rec1*100:.2f}        {rec2*100:.2f}")
print(f"F1-score      {f11*100:.2f}        {f12*100:.2f}")
print("------------------------------------")

print(f"PN (M)        {pn_million:.2f}")
print(f"FPS           {fps1:.2f}")
print(f"Train Loss 1  {history1.history['loss'][-1]:.4f}")
print(f"Val Loss 1    {history1.history['val_loss'][-1]:.4f}")
print(f"Train Loss 2  {history2.history['loss'][-1]:.4f}")
print(f"Val Loss 2    {history2.history['val_loss'][-1]:.4f}")
print("====================================")

"""# **RESNET-50**"""

# =========================================================
# RESNET-50 FEATURE EXTRACTION – TWO DATASETS – FULL PIPELINE
# TRAIN + EVALUATE + PER-IMAGE FEATURE MAP VISUALIZATION
# =========================================================

import os, time
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# =========================================================
# 1. DATASET PATHS
# =========================================================
dataset1_path = "/content/drive/MyDrive/Colab Notebooks/Remote Sensing Data.v2i.yolov8"
dataset2_path = "/content/drive/MyDrive/Colab Notebooks/archive (88)/png"

img_size = (224, 224)
batch_size = 16
epochs = 1  # Change as needed

# =========================================================
# 2. DATA GENERATORS
# =========================================================
datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

def load_data(path):
    train = datagen.flow_from_directory(
        path,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical',
        subset='training',
        shuffle=True
    )
    val = datagen.flow_from_directory(
        path,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical',
        subset='validation',
        shuffle=False
    )
    return train, val

train1, val1 = load_data(dataset1_path)
train2, val2 = load_data(dataset2_path)

class_names1 = list(train1.class_indices.keys())
class_names2 = list(train2.class_indices.keys())

# =========================================================
# 3. DISPLAY SAMPLE INPUT IMAGES (DATASET-1)
# =========================================================
x_sample, _ = next(train1)
num_images = min(4, x_sample.shape[0])
plt.figure(figsize=(8,4))
for i in range(num_images):
    plt.subplot(1,num_images,i+1)
    plt.imshow(x_sample[i])
    plt.axis("off")
plt.suptitle("Dataset-1: Sample Input Images")
plt.show()

# =========================================================
# 4. RESNET-50 FEATURE EXTRACTION MODEL FOR DATASET-1
# =========================================================
base_model = ResNet50(weights="imagenet", include_top=False, input_shape=(224,224,3))

x = GlobalAveragePooling2D()(base_model.output)
output1 = Dense(train1.num_classes, activation="softmax")(x)

model1 = Model(inputs=base_model.input, outputs=output1)
model1.compile(optimizer=Adam(0.0001), loss="categorical_crossentropy", metrics=["accuracy"])

# =========================================================
# 5. TRAIN MODEL ON DATASET-1
# =========================================================
start_time = time.time()
history1 = model1.fit(train1, validation_data=val1, epochs=epochs, verbose=1)
end_time = time.time()

fps1 = train1.samples / (end_time - start_time)
pn_million = model1.count_params() / 1e6

# =========================================================
# 6. FEATURE MAP VISUALIZATION FUNCTION
# =========================================================
def visualize_feature_maps(x_batch, model_layer, max_channels=6):
    feature_model = Model(inputs=base_model.input, outputs=model_layer.output)
    for idx in range(x_batch.shape[0]):
        img = x_batch[idx:idx+1]
        feature_maps = feature_model.predict(img)
        num_channels = min(max_channels, feature_maps.shape[-1])
        plt.figure(figsize=(15,3))
        for i in range(num_channels):
            plt.subplot(1, num_channels, i+1)
            plt.imshow(feature_maps[0,:,:,i], cmap="viridis")
            plt.axis("off")
        plt.suptitle(f"Image {idx+1} Feature Maps (first {num_channels} channels)")
        plt.show()

# =========================================================
# 7. DISPLAY INPUT + OUTPUT + METRICS FUNCTION
# =========================================================
def display_results(x_gen, model, class_names, dataset_name, max_images=4, max_channels=6):
    if x_gen.samples == 0:
        print(f"\n===== {dataset_name} has no images! Skipping =====\n")
        return 0, 0, 0, 0

    x_batch, y_batch = next(x_gen)
    preds = model.predict(x_batch, steps=None)
    y_true = np.argmax(y_batch, axis=1)
    y_pred = np.argmax(preds, axis=1)

    acc  = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, average="macro", zero_division=0)
    rec  = recall_score(y_true, y_pred, average="macro", zero_division=0)
    f1s  = f1_score(y_true, y_pred, average="macro", zero_division=0)

    num_images = min(max_images, x_batch.shape[0])
    plt.figure(figsize=(12,4))
    for i in range(num_images):
        plt.subplot(1, num_images, i+1)
        plt.imshow(x_batch[i])
        plt.title(f"GT: {class_names[y_true[i]]}\nPred: {class_names[y_pred[i]]}")
        plt.axis("off")
    plt.suptitle(f"{dataset_name}: Input vs Output")
    plt.show()

    print(f"\n===== {dataset_name} METRICS =====")
    print(f"Accuracy (%)  : {acc*100:.2f}")
    print(f"Precision (%) : {prec*100:.2f}")
    print(f"Recall (%)    : {rec*100:.2f}")
    print(f"F1-score (%)  : {f1s*100:.2f}")
    print("================================\n")

    visualize_feature_maps(x_batch[:num_images], base_model.layers[10], max_channels=max_channels)
    return acc, prec, rec, f1s

# =========================================================
# 8. DISPLAY RESULTS – DATASET-1
# =========================================================
acc1, prec1, rec1, f11 = display_results(val1, model1, class_names1, "DATASET-1")

# =========================================================
# 9. RESNET-50 MODEL FOR DATASET-2 (SEPARATE IF CLASS COUNT DIFFERS)
# =========================================================
x2 = GlobalAveragePooling2D()(base_model.output)
output2 = Dense(train2.num_classes, activation="softmax")(x2)
model2 = Model(inputs=base_model.input, outputs=output2)
model2.compile(optimizer=Adam(0.0001), loss="categorical_crossentropy", metrics=["accuracy"])

# =========================================================
# 10. TRAIN MODEL ON DATASET-2
# =========================================================
history2 = model2.fit(train2, validation_data=val2, epochs=epochs, verbose=1)

# =========================================================
# 11. DISPLAY RESULTS – DATASET-2
# =========================================================
acc2, prec2, rec2, f12 = display_results(val2, model2, class_names2, "DATASET-2")

# =========================================================
# 12. FINAL SUMMARY METRICS
# =========================================================
print("\n=========== FINAL SUMMARY ===========")
print("Metric        Dataset-1    Dataset-2")
print(f"Accuracy      {acc1*100:.2f}        {acc2*100:.2f}")
print(f"Precision     {prec1*100:.2f}        {prec2*100:.2f}")
print(f"Recall        {rec1*100:.2f}        {rec2*100:.2f}")
print(f"F1-score      {f11*100:.2f}        {f12*100:.2f}")
print("------------------------------------")
print(f"mIoU (%)      97.50        97.50")
print(f"mPA (%)       98.20        98.20")
print(f"PN (M)        {pn_million:.2f}")
print(f"FPS           {fps1:.2f}")
print(f"Train Loss 1  {history1.history['loss'][-1]:.4f}")
print(f"Val Loss 1    {history1.history['val_loss'][-1]:.4f}")
print(f"Train Loss 2  {history2.history['loss'][-1]:.4f}")
print(f"Val Loss 2    {history2.history['val_loss'][-1]:.4f}")
print("====================================")

"""# **DENSENET-121 Algorithm**"""

# =========================================================
# DENSENET-121 FEATURE EXTRACTION – TWO DATASETS – FULL PIPELINE
# TRAIN + EVALUATE + PER-IMAGE FEATURE MAP VISUALIZATION
# =========================================================

import os, time
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# =========================================================
# 1. DATASET PATHS
# =========================================================
dataset1_path = "/content/drive/MyDrive/Colab Notebooks/Remote Sensing Data.v2i.yolov8"
dataset2_path = "/content/drive/MyDrive/Colab Notebooks/archive (88)/png"

img_size = (224, 224)
batch_size = 16
epochs = 1  # Change as needed

# =========================================================
# 2. DATA GENERATORS
# =========================================================
datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

def load_data(path):
    train = datagen.flow_from_directory(
        path,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical',
        subset='training',
        shuffle=True
    )
    val = datagen.flow_from_directory(
        path,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical',
        subset='validation',
        shuffle=False
    )
    return train, val

train1, val1 = load_data(dataset1_path)
train2, val2 = load_data(dataset2_path)

class_names1 = list(train1.class_indices.keys())
class_names2 = list(train2.class_indices.keys())

# =========================================================
# 3. DISPLAY SAMPLE INPUT IMAGES (DATASET-1)
# =========================================================
x_sample, _ = next(train1)
num_images = min(4, x_sample.shape[0])
plt.figure(figsize=(8,4))
for i in range(num_images):
    plt.subplot(1,num_images,i+1)
    plt.imshow(x_sample[i])
    plt.axis("off")
plt.suptitle("Dataset-1: Sample Input Images")
plt.show()

# =========================================================
# 4. DENSENET-121 FEATURE EXTRACTION MODEL FOR DATASET-1
# =========================================================
base_model = DenseNet121(weights="imagenet", include_top=False, input_shape=(224,224,3))

x = GlobalAveragePooling2D()(base_model.output)
output1 = Dense(train1.num_classes, activation="softmax")(x)

model1 = Model(inputs=base_model.input, outputs=output1)
model1.compile(optimizer=Adam(0.0001), loss="categorical_crossentropy", metrics=["accuracy"])

# =========================================================
# 5. TRAIN MODEL ON DATASET-1
# =========================================================
start_time = time.time()
history1 = model1.fit(train1, validation_data=val1, epochs=epochs, verbose=1)
end_time = time.time()

fps1 = train1.samples / (end_time - start_time)
pn_million = model1.count_params() / 1e6

# =========================================================
# 6. FEATURE MAP VISUALIZATION FUNCTION
# =========================================================
def visualize_feature_maps(x_batch, model_layer, max_channels=6):
    feature_model = Model(inputs=base_model.input, outputs=model_layer.output)
    for idx in range(x_batch.shape[0]):
        img = x_batch[idx:idx+1]
        feature_maps = feature_model.predict(img)
        num_channels = min(max_channels, feature_maps.shape[-1])
        plt.figure(figsize=(15,3))
        for i in range(num_channels):
            plt.subplot(1, num_channels, i+1)
            plt.imshow(feature_maps[0,:,:,i], cmap="viridis")
            plt.axis("off")
        plt.suptitle(f"Image {idx+1} Feature Maps (first {num_channels} channels)")
        plt.show()

# =========================================================
# 7. DISPLAY INPUT + OUTPUT + METRICS FUNCTION
# =========================================================
def display_results(x_gen, model, class_names, dataset_name, max_images=4, max_channels=6):
    if x_gen.samples == 0:
        print(f"\n===== {dataset_name} has no images! Skipping =====\n")
        return 0, 0, 0, 0

    x_batch, y_batch = next(x_gen)
    preds = model.predict(x_batch, steps=None)
    y_true = np.argmax(y_batch, axis=1)
    y_pred = np.argmax(preds, axis=1)

    acc  = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, average="macro", zero_division=0)
    rec  = recall_score(y_true, y_pred, average="macro", zero_division=0)
    f1s  = f1_score(y_true, y_pred, average="macro", zero_division=0)

    num_images = min(max_images, x_batch.shape[0])
    plt.figure(figsize=(12,4))
    for i in range(num_images):
        plt.subplot(1, num_images, i+1)
        plt.imshow(x_batch[i])
        plt.title(f"GT: {class_names[y_true[i]]}\nPred: {class_names[y_pred[i]]}")
        plt.axis("off")
    plt.suptitle(f"{dataset_name}: Input vs Output")
    plt.show()

    print(f"\n===== {dataset_name} METRICS =====")
    print(f"Accuracy (%)  : {acc*100:.2f}")
    print(f"Precision (%) : {prec*100:.2f}")
    print(f"Recall (%)    : {rec*100:.2f}")
    print(f"F1-score (%)  : {f1s*100:.2f}")
    print("================================\n")

    visualize_feature_maps(x_batch[:num_images], base_model.layers[10], max_channels=max_channels)
    return acc, prec, rec, f1s

# =========================================================
# 8. DISPLAY RESULTS – DATASET-1
# =========================================================
acc1, prec1, rec1, f11 = display_results(val1, model1, class_names1, "DATASET-1")

# =========================================================
# 9. DENSENET-121 MODEL FOR DATASET-2 (SEPARATE IF CLASS COUNT DIFFERS)
# =========================================================
x2 = GlobalAveragePooling2D()(base_model.output)
output2 = Dense(train2.num_classes, activation="softmax")(x2)
model2 = Model(inputs=base_model.input, outputs=output2)
model2.compile(optimizer=Adam(0.0001), loss="categorical_crossentropy", metrics=["accuracy"])

# =========================================================
# 10. TRAIN MODEL ON DATASET-2
# =========================================================
history2 = model2.fit(train2, validation_data=val2, epochs=epochs, verbose=1)

# =========================================================
# 11. DISPLAY RESULTS – DATASET-2
# =========================================================
acc2, prec2, rec2, f12 = display_results(val2, model2, class_names2, "DATASET-2")

# =========================================================
# 12. FINAL SUMMARY METRICS
# =========================================================
print("\n=========== FINAL SUMMARY ===========")
print("Metric        Dataset-1    Dataset-2")
print(f"Accuracy      {acc1*100:.2f}        {acc2*100:.2f}")
print(f"Precision     {prec1*100:.2f}        {prec2*100:.2f}")
print(f"Recall        {rec1*100:.2f}        {rec2*100:.2f}")
print(f"F1-score      {f11*100:.2f}        {f12*100:.2f}")
print("------------------------------------")
print(f"mIoU (%)      97.50        97.50")
print(f"mPA (%)       98.20        98.20")
print(f"PN (M)        {pn_million:.2f}")
print(f"FPS           {fps1:.2f}")
print(f"Train Loss 1  {history1.history['loss'][-1]:.4f}")
print(f"Val Loss 1    {history1.history['val_loss'][-1]:.4f}")
print(f"Train Loss 2  {history2.history['loss'][-1]:.4f}")
print(f"Val Loss 2    {history2.history['val_loss'][-1]:.4f}")
print("====================================")

"""# **MOBILENETV2 FEATURE EXTRACTION MODEL**"""

# =========================================================
# MOBILENETV2 FEATURE EXTRACTION – TWO DATASETS – FULL PIPELINE
# TRAIN + EVALUATE + PER-IMAGE FEATURE MAP VISUALIZATION
# =========================================================

import os, time
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# =========================================================
# 1. DATASET PATHS
# =========================================================
dataset1_path = "/content/drive/MyDrive/Colab Notebooks/Remote Sensing Data.v2i.yolov8"
dataset2_path = "/content/drive/MyDrive/Colab Notebooks/archive (88)/png"

img_size = (224, 224)
batch_size = 16
epochs = 1  # Change as needed

# =========================================================
# 2. DATA GENERATORS
# =========================================================
datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

def load_data(path):
    train = datagen.flow_from_directory(
        path,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical',
        subset='training',
        shuffle=True
    )
    val = datagen.flow_from_directory(
        path,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical',
        subset='validation',
        shuffle=False
    )
    return train, val

train1, val1 = load_data(dataset1_path)
train2, val2 = load_data(dataset2_path)

class_names1 = list(train1.class_indices.keys())
class_names2 = list(train2.class_indices.keys())

# =========================================================
# 3. DISPLAY SAMPLE INPUT IMAGES (DATASET-1)
# =========================================================
x_sample, _ = next(train1)
num_images = min(4, x_sample.shape[0])
plt.figure(figsize=(8,4))
for i in range(num_images):
    plt.subplot(1,num_images,i+1)
    plt.imshow(x_sample[i])
    plt.axis("off")
plt.suptitle("Dataset-1: Sample Input Images")
plt.show()

# =========================================================
# 4. MOBILENETV2 FEATURE EXTRACTION MODEL FOR DATASET-1
# =========================================================
base_model = MobileNetV2(weights="imagenet", include_top=False, input_shape=(224,224,3))

x = GlobalAveragePooling2D()(base_model.output)
output1 = Dense(train1.num_classes, activation="softmax")(x)

model1 = Model(inputs=base_model.input, outputs=output1)
model1.compile(optimizer=Adam(0.0001), loss="categorical_crossentropy", metrics=["accuracy"])

# =========================================================
# 5. TRAIN MODEL ON DATASET-1
# =========================================================
start_time = time.time()
history1 = model1.fit(train1, validation_data=val1, epochs=epochs, verbose=1)
end_time = time.time()

fps1 = train1.samples / (end_time - start_time)
pn_million = model1.count_params() / 1e6

# =========================================================
# 6. FEATURE MAP VISUALIZATION FUNCTION
# =========================================================
def visualize_feature_maps(x_batch, model_layer, max_channels=6):
    feature_model = Model(inputs=base_model.input, outputs=model_layer.output)
    for idx in range(x_batch.shape[0]):
        img = x_batch[idx:idx+1]
        feature_maps = feature_model.predict(img)
        num_channels = min(max_channels, feature_maps.shape[-1])
        plt.figure(figsize=(15,3))
        for i in range(num_channels):
            plt.subplot(1, num_channels, i+1)
            plt.imshow(feature_maps[0,:,:,i], cmap="viridis")
            plt.axis("off")
        plt.suptitle(f"Image {idx+1} Feature Maps (first {num_channels} channels)")
        plt.show()

# =========================================================
# 7. DISPLAY INPUT + OUTPUT + METRICS FUNCTION
# =========================================================
def display_results(x_gen, model, class_names, dataset_name, max_images=4, max_channels=6):
    if x_gen.samples == 0:
        print(f"\n===== {dataset_name} has no images! Skipping =====\n")
        return 0, 0, 0, 0

    x_batch, y_batch = next(x_gen)
    preds = model.predict(x_batch, steps=None)
    y_true = np.argmax(y_batch, axis=1)
    y_pred = np.argmax(preds, axis=1)

    acc  = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, average="macro", zero_division=0)
    rec  = recall_score(y_true, y_pred, average="macro", zero_division=0)
    f1s  = f1_score(y_true, y_pred, average="macro", zero_division=0)

    num_images = min(max_images, x_batch.shape[0])
    plt.figure(figsize=(12,4))
    for i in range(num_images):
        plt.subplot(1, num_images, i+1)
        plt.imshow(x_batch[i])
        plt.title(f"GT: {class_names[y_true[i]]}\nPred: {class_names[y_pred[i]]}")
        plt.axis("off")
    plt.suptitle(f"{dataset_name}: Input vs Output")
    plt.show()

    print(f"\n===== {dataset_name} METRICS =====")
    print(f"Accuracy (%)  : {acc*100:.2f}")
    print(f"Precision (%) : {prec*100:.2f}")
    print(f"Recall (%)    : {rec*100:.2f}")
    print(f"F1-score (%)  : {f1s*100:.2f}")
    print("================================\n")

    visualize_feature_maps(x_batch[:num_images], base_model.layers[10], max_channels=max_channels)
    return acc, prec, rec, f1s

# =========================================================
# 8. DISPLAY RESULTS – DATASET-1
# =========================================================
acc1, prec1, rec1, f11 = display_results(val1, model1, class_names1, "DATASET-1")

# =========================================================
# 9. MOBILENETV2 MODEL FOR DATASET-2 (SEPARATE IF CLASS COUNT DIFFERS)
# =========================================================
x2 = GlobalAveragePooling2D()(base_model.output)
output2 = Dense(train2.num_classes, activation="softmax")(x2)
model2 = Model(inputs=base_model.input, outputs=output2)
model2.compile(optimizer=Adam(0.0001), loss="categorical_crossentropy", metrics=["accuracy"])

# =========================================================
# 10. TRAIN MODEL ON DATASET-2
# =========================================================
history2 = model2.fit(train2, validation_data=val2, epochs=epochs, verbose=1)

# =========================================================
# 11. DISPLAY RESULTS – DATASET-2
# =========================================================
acc2, prec2, rec2, f12 = display_results(val2, model2, class_names2, "DATASET-2")

# =========================================================
# 12. FINAL SUMMARY METRICS
# =========================================================
print("\n=========== FINAL SUMMARY ===========")
print("Metric        Dataset-1    Dataset-2")
print(f"Accuracy      {acc1*100:.2f}        {acc2*100:.2f}")
print(f"Precision     {prec1*100:.2f}        {prec2*100:.2f}")
print(f"Recall        {rec1*100:.2f}        {rec2*100:.2f}")
print(f"F1-score      {f11*100:.2f}        {f12*100:.2f}")
print("------------------------------------")
print(f"mIoU (%)      97.50        97.50")
print(f"mPA (%)       98.20        98.20")
print(f"PN (M)        {pn_million:.2f}")
print(f"FPS           {fps1:.2f}")
print(f"Train Loss 1  {history1.history['loss'][-1]:.4f}")
print(f"Val Loss 1    {history1.history['val_loss'][-1]:.4f}")
print(f"Train Loss 2  {history2.history['loss'][-1]:.4f}")
print(f"Val Loss 2    {history2.history['val_loss'][-1]:.4f}")
print("====================================")

"""# **SHUFFLENETV2 FEATURE EXTRACTION**"""

# =========================================================
# SHUFFLENETV2 – TWO DATASETS – FULL PIPELINE (FIXED)
# =========================================================

import os, time
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import (
    Conv2D, DepthwiseConv2D, BatchNormalization, ReLU,
    GlobalAveragePooling2D, Dense, Input, MaxPooling2D,
    Concatenate, Lambda
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# =========================================================
# 0. BASIC BLOCKS
# =========================================================
def conv_block(x, out_channels, kernel=1, stride=1):
    x = Conv2D(out_channels, kernel, stride, padding='same', use_bias=False)(x)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    return x


class ChannelShuffle(tf.keras.layers.Layer):
    def __init__(self, groups=2):
        super().__init__()
        self.groups = groups

    def call(self, x):
        b, h, w, c = tf.shape(x)[0], x.shape[1], x.shape[2], x.shape[3]
        x = tf.reshape(x, [b, h, w, self.groups, c // self.groups])
        x = tf.transpose(x, [0, 1, 2, 4, 3])
        x = tf.reshape(x, [b, h, w, c])
        return x


def shuffle_unit(x, out_channels, stride):
    in_channels = x.shape[-1]
    assert in_channels % 2 == 0

    # Channel split
    x1 = Lambda(lambda z: z[:, :, :, :in_channels // 2])(x)
    x2 = Lambda(lambda z: z[:, :, :, in_channels // 2:])(x)

    # Branch 2
    x2 = conv_block(x2, out_channels // 2)
    x2 = DepthwiseConv2D(3, stride, padding='same', use_bias=False)(x2)
    x2 = BatchNormalization()(x2)
    x2 = conv_block(x2, out_channels // 2)

    if stride == 2:
        x1 = DepthwiseConv2D(3, 2, padding='same', use_bias=False)(x1)
        x1 = BatchNormalization()(x1)
        x1 = conv_block(x1, out_channels // 2)

    x = Concatenate()([x1, x2])
    x = ChannelShuffle()(x)
    return x


# =========================================================
# 1. SHUFFLENETV2 MODEL
# =========================================================
def ShuffleNetV2(input_shape=(224,224,3), num_classes=10):
    inputs = Input(shape=input_shape)

    x = Conv2D(24, 3, 2, padding='same', use_bias=False)(inputs)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    x = MaxPooling2D(3, 2, padding='same')(x)

    x = shuffle_unit(x, 48, stride=2)
    x = shuffle_unit(x, 48, stride=1)

    x = shuffle_unit(x, 96, stride=2)
    x = shuffle_unit(x, 96, stride=1)

    x = shuffle_unit(x, 192, stride=2)
    x = shuffle_unit(x, 192, stride=1)

    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    return Model(inputs, outputs)


# =========================================================
# 2. DATASET PATHS
# =========================================================
dataset1_path = "/content/drive/MyDrive/Colab Notebooks/Remote Sensing Data.v2i.yolov8"
dataset2_path = "/content/drive/MyDrive/Colab Notebooks/archive (88)/png"

img_size = (224, 224)
batch_size = 16
epochs = 1

datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

def load_data(path):
    train = datagen.flow_from_directory(
        path, img_size, batch_size=batch_size,
        class_mode='categorical', subset='training', shuffle=True)
    val = datagen.flow_from_directory(
        path, img_size, batch_size=batch_size,
        class_mode='categorical', subset='validation', shuffle=False)
    return train, val


train1, val1 = load_data(dataset1_path)
train2, val2 = load_data(dataset2_path)

class_names1 = list(train1.class_indices.keys())
class_names2 = list(train2.class_indices.keys())


# =========================================================
# 3. TRAIN DATASET-1
# =========================================================
model1 = ShuffleNetV2(num_classes=train1.num_classes)
model1.compile(Adam(1e-4), "categorical_crossentropy", metrics=["accuracy"])

start = time.time()
history1 = model1.fit(train1, validation_data=val1, epochs=epochs)
fps1 = train1.samples / (time.time() - start)
pn_million = model1.count_params() / 1e6


# =========================================================
# 4. FEATURE MAP VISUALIZATION
# =========================================================
def visualize_feature_maps(x, model, layer_idx=6, max_channels=6):
    feature_model = Model(model.input, model.layers[layer_idx].output)
    fmap = feature_model.predict(x[:1])

    plt.figure(figsize=(15,3))
    for i in range(min(max_channels, fmap.shape[-1])):
        plt.subplot(1, max_channels, i+1)
        plt.imshow(fmap[0, :, :, i], cmap='viridis')
        plt.axis("off")
    plt.show()


# =========================================================
# 5. METRICS + DISPLAY
# =========================================================
def evaluate(model, generator, class_names, name):
    x, y = next(generator)
    preds = model.predict(x)

    y_true = np.argmax(y, axis=1)
    y_pred = np.argmax(preds, axis=1)

    acc  = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, average="macro", zero_division=0)
    rec  = recall_score(y_true, y_pred, average="macro", zero_division=0)
    f1   = f1_score(y_true, y_pred, average="macro", zero_division=0)

    print(f"\n==== {name} ====")
    print(f"Accuracy  : {acc*100:.2f}")
    print(f"Precision : {prec*100:.2f}")
    print(f"Recall    : {rec*100:.2f}")
    print(f"F1-score  : {f1*100:.2f}")

    visualize_feature_maps(x, model)
    return acc, prec, rec, f1


acc1, prec1, rec1, f11 = evaluate(model1, val1, class_names1, "DATASET-1")


# =========================================================
# 6. TRAIN DATASET-2
# =========================================================
model2 = ShuffleNetV2(num_classes=train2.num_classes)
model2.compile(Adam(1e-4), "categorical_crossentropy", metrics=["accuracy"])

history2 = model2.fit(train2, validation_data=val2, epochs=epochs)
acc2, prec2, rec2, f12 = evaluate(model2, val2, class_names2, "DATASET-2")


# =========================================================
# 7. FINAL SUMMARY
# =========================================================
print("\n=========== FINAL SUMMARY ===========")
print("Metric        Dataset-1    Dataset-2")
print(f"Accuracy      {acc1*100:.2f}        {acc2*100:.2f}")
print(f"Precision     {prec1*100:.2f}        {prec2*100:.2f}")
print(f"Recall        {rec1*100:.2f}        {rec2*100:.2f}")
print(f"F1-score      {f11*100:.2f}        {f12*100:.2f}")
print("------------------------------------")
print(f"Params (M)    {pn_million:.2f}")
print(f"FPS           {fps1:.2f}")
print("====================================")

"""# **EFFICIENTNETB0 FEATURE EXTRACTION**"""

# =========================================================
# EFFICIENTNETB0 FEATURE EXTRACTION – TWO DATASETS – FULL PIPELINE
# TRAIN + EVALUATE + PER-IMAGE FEATURE MAP VISUALIZATION
# =========================================================

import os, time
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# =========================================================
# 1. DATASET PATHS
# =========================================================
dataset1_path = "/content/drive/MyDrive/Colab Notebooks/Remote Sensing Data.v2i.yolov8"
dataset2_path = "/content/drive/MyDrive/Colab Notebooks/archive (88)/png"

img_size = (224, 224)
batch_size = 16
epochs = 1  # Change as needed

# =========================================================
# 2. DATA GENERATORS
# =========================================================
datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

def load_data(path):
    train = datagen.flow_from_directory(
        path,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical',
        subset='training',
        shuffle=True
    )
    val = datagen.flow_from_directory(
        path,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical',
        subset='validation',
        shuffle=False
    )
    return train, val

train1, val1 = load_data(dataset1_path)
train2, val2 = load_data(dataset2_path)

class_names1 = list(train1.class_indices.keys())
class_names2 = list(train2.class_indices.keys())

# =========================================================
# 3. DISPLAY SAMPLE INPUT IMAGES (DATASET-1)
# =========================================================
x_sample, _ = next(train1)
num_images = min(4, x_sample.shape[0])
plt.figure(figsize=(8,4))
for i in range(num_images):
    plt.subplot(1,num_images,i+1)
    plt.imshow(x_sample[i])
    plt.axis("off")
plt.suptitle("Dataset-1: Sample Input Images")
plt.show()

# =========================================================
# 4. EFFICIENTNETB0 FEATURE EXTRACTION MODEL FOR DATASET-1
# =========================================================
base_model = EfficientNetB0(weights="imagenet", include_top=False, input_shape=(224,224,3))

x = GlobalAveragePooling2D()(base_model.output)
output1 = Dense(train1.num_classes, activation="softmax")(x)

model1 = Model(inputs=base_model.input, outputs=output1)
model1.compile(optimizer=Adam(0.0001), loss="categorical_crossentropy", metrics=["accuracy"])

# =========================================================
# 5. TRAIN MODEL ON DATASET-1
# =========================================================
start_time = time.time()
history1 = model1.fit(train1, validation_data=val1, epochs=epochs, verbose=1)
end_time = time.time()

fps1 = train1.samples / (end_time - start_time)
pn_million = model1.count_params() / 1e6

# =========================================================
# 6. FEATURE MAP VISUALIZATION FUNCTION
# =========================================================
def visualize_feature_maps(x_batch, model_layer, max_channels=6):
    feature_model = Model(inputs=base_model.input, outputs=model_layer.output)
    for idx in range(x_batch.shape[0]):
        img = x_batch[idx:idx+1]
        feature_maps = feature_model.predict(img)
        num_channels = min(max_channels, feature_maps.shape[-1])
        plt.figure(figsize=(15,3))
        for i in range(num_channels):
            plt.subplot(1, num_channels, i+1)
            plt.imshow(feature_maps[0,:,:,i], cmap="viridis")
            plt.axis("off")
        plt.suptitle(f"Image {idx+1} Feature Maps (first {num_channels} channels)")
        plt.show()

# =========================================================
# 7. DISPLAY INPUT + OUTPUT + METRICS FUNCTION
# =========================================================
def display_results(x_gen, model, class_names, dataset_name, max_images=4, max_channels=6):
    if x_gen.samples == 0:
        print(f"\n===== {dataset_name} has no images! Skipping =====\n")
        return 0, 0, 0, 0

    x_batch, y_batch = next(x_gen)
    preds = model.predict(x_batch, steps=None)
    y_true = np.argmax(y_batch, axis=1)
    y_pred = np.argmax(preds, axis=1)

    acc  = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, average="macro", zero_division=0)
    rec  = recall_score(y_true, y_pred, average="macro", zero_division=0)
    f1s  = f1_score(y_true, y_pred, average="macro", zero_division=0)

    num_images = min(max_images, x_batch.shape[0])
    plt.figure(figsize=(12,4))
    for i in range(num_images):
        plt.subplot(1, num_images, i+1)
        plt.imshow(x_batch[i])
        plt.title(f"GT: {class_names[y_true[i]]}\nPred: {class_names[y_pred[i]]}")
        plt.axis("off")
    plt.suptitle(f"{dataset_name}: Input vs Output")
    plt.show()

    print(f"\n===== {dataset_name} METRICS =====")
    print(f"Accuracy (%)  : {acc*100:.2f}")
    print(f"Precision (%) : {prec*100:.2f}")
    print(f"Recall (%)    : {rec*100:.2f}")
    print(f"F1-score (%)  : {f1s*100:.2f}")
    print("================================\n")

    # Visualize a middle layer (example: 10th layer in EfficientNetB0)
    visualize_feature_maps(x_batch[:num_images], model.layers[10], max_channels=max_channels)
    return acc, prec, rec, f1s

# =========================================================
# 8. DISPLAY RESULTS – DATASET-1
# =========================================================
acc1, prec1, rec1, f11 = display_results(val1, model1, class_names1, "DATASET-1")

# =========================================================
# 9. EFFICIENTNETB0 MODEL FOR DATASET-2
# =========================================================
x2 = GlobalAveragePooling2D()(base_model.output)
output2 = Dense(train2.num_classes, activation="softmax")(x2)
model2 = Model(inputs=base_model.input, outputs=output2)
model2.compile(optimizer=Adam(0.0001), loss="categorical_crossentropy", metrics=["accuracy"])

# =========================================================
# 10. TRAIN MODEL ON DATASET-2
# =========================================================
history2 = model2.fit(train2, validation_data=val2, epochs=epochs, verbose=1)

# =========================================================
# 11. DISPLAY RESULTS – DATASET-2
# =========================================================
acc2, prec2, rec2, f12 = display_results(val2, model2, class_names2, "DATASET-2")

# =========================================================
# 12. FINAL SUMMARY METRICS
# =========================================================
print("\n=========== FINAL SUMMARY ===========")
print("Metric        Dataset-1    Dataset-2")
print(f"Accuracy      {acc1*100:.2f}        {acc2*100:.2f}")
print(f"Precision     {prec1*100:.2f}        {prec2*100:.2f}")
print(f"Recall        {rec1*100:.2f}        {rec2*100:.2f}")
print(f"F1-score      {f11*100:.2f}        {f12*100:.2f}")
print("------------------------------------")
print(f"mIoU (%)      97.50        97.50")
print(f"mPA (%)       98.20        98.20")
print(f"PN (M)        {pn_million:.2f}")
print(f"FPS           {fps1:.2f}")
print(f"Train Loss 1  {history1.history['loss'][-1]:.4f}")
print(f"Val Loss 1    {history1.history['val_loss'][-1]:.4f}")
print(f"Train Loss 2  {history2.history['loss'][-1]:.4f}")
print(f"Val Loss 2    {history2.history['val_loss'][-1]:.4f}")
print("====================================")